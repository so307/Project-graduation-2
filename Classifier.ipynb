{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f52bcfe9-7334-4025-8d79-7efde6d37cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing libraries\n",
    "import pandas as pd  #Data manipulation\n",
    "import string  #Remove punctuation & characters\n",
    "import nltk  #Natural language processing \n",
    "import pickle  #For loading saved models and vectorizers\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords  #Stop word removal\n",
    "from nltk.tokenize import word_tokenize  #Tokenizition\n",
    "from nltk.stem import WordNetLemmatizer  #Import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet  #Import WordNet\n",
    "\n",
    "#Feature extractions libraries\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#Models libraries\n",
    "from sklearn.model_selection import train_test_split #For data splitting\n",
    "#Model Evaluation Function\n",
    "from sklearn.metrics import accuracy_score, classification_report  #Import metrics\n",
    "from sklearn.svm import SVC  #SVM Model\n",
    "from sklearn.naive_bayes import MultinomialNB #Naive Bayes Model -  MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest Model\n",
    "\n",
    "#to arabic classification\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pyarabic.araby as araby\n",
    "import qalsadi.lemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc97ae55-fb06-4ff4-bf04-aa1447742356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05942943-4ee5-4e88-9755-c4e2e6579cb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>['tv', 'future', 'hands', 'viewers', 'home', '...</td>\n",
       "      <td>['tv', 'future', 'hand', 'viewer', 'home', 'th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>['worldcom', 'boss', 'left', 'books', 'alone',...</td>\n",
       "      <td>['worldcom', 'bos', 'leave', 'book', 'alone', ...</td>\n",
       "      <td>worldcom bos leave book alone former worldcom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>['tigers', 'wary', 'farrell', 'gamble', 'leice...</td>\n",
       "      <td>['tiger', 'wary', 'farrell', 'gamble', 'leices...</td>\n",
       "      <td>tiger wary farrell gamble leicester say rush m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>['ocean', 'twelve', 'raids', 'box', 'office', ...</td>\n",
       "      <td>['ocean', 'twelve', 'raid', 'box', 'office', '...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       category  \\\n",
       "0           0           tech   \n",
       "1           1       business   \n",
       "2           2          sport   \n",
       "3           3          sport   \n",
       "4           4  entertainment   \n",
       "\n",
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['tv', 'future', 'hands', 'viewers', 'home', '...   \n",
       "1  ['worldcom', 'boss', 'left', 'books', 'alone',...   \n",
       "2  ['tigers', 'wary', 'farrell', 'gamble', 'leice...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raids', 'box', 'office', ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['tv', 'future', 'hand', 'viewer', 'home', 'th...   \n",
       "1  ['worldcom', 'bos', 'leave', 'book', 'alone', ...   \n",
       "2  ['tiger', 'wary', 'farrell', 'gamble', 'leices...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raid', 'box', 'office', '...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos leave book alone former worldcom ...  \n",
       "2  tiger wary farrell gamble leicester say rush m...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read excel file\n",
    "data = pd.read_csv(r'C:\\Users\\s7r_2\\Downloads\\bbc-text (1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "98cb2979-efe9-4bba-a572-21aeb48bdcec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>['tv', 'future', 'hands', 'viewers', 'home', '...</td>\n",
       "      <td>['tv', 'future', 'hand', 'viewer', 'home', 'th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>['worldcom', 'boss', 'left', 'books', 'alone',...</td>\n",
       "      <td>['worldcom', 'bos', 'leave', 'book', 'alone', ...</td>\n",
       "      <td>worldcom bos leave book alone former worldcom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>['tigers', 'wary', 'farrell', 'gamble', 'leice...</td>\n",
       "      <td>['tiger', 'wary', 'farrell', 'gamble', 'leices...</td>\n",
       "      <td>tiger wary farrell gamble leicester say rush m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>['ocean', 'twelve', 'raids', 'box', 'office', ...</td>\n",
       "      <td>['ocean', 'twelve', 'raid', 'box', 'office', '...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       category  \\\n",
       "0           0           tech   \n",
       "1           1       business   \n",
       "2           2          sport   \n",
       "3           3          sport   \n",
       "4           4  entertainment   \n",
       "\n",
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['tv', 'future', 'hands', 'viewers', 'home', '...   \n",
       "1  ['worldcom', 'boss', 'left', 'books', 'alone',...   \n",
       "2  ['tigers', 'wary', 'farrell', 'gamble', 'leice...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raids', 'box', 'office', ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['tv', 'future', 'hand', 'viewer', 'home', 'th...   \n",
       "1  ['worldcom', 'bos', 'leave', 'book', 'alone', ...   \n",
       "2  ['tiger', 'wary', 'farrell', 'gamble', 'leices...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raid', 'box', 'office', '...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos leave book alone former worldcom ...  \n",
       "2  tiger wary farrell gamble leicester say rush m...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# دالة لإزالة الرموز غير ASCII\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "# دالة لتنظيف النصوص\n",
    "def clean_text(text):\n",
    "    # تأكد من أن المتغير هو نص\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# تحويل قائمة الكلمات إلى نص مفرد\n",
    "def list_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# تأكد من وجود عمود 'tokens' كقوائم\n",
    "# تطبيق دالة تحويل القوائم إلى نصوص ثم دالة التنظيف على عمود tokens\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# عرض أول خمس صفوف من DataFrame بعد التنظيف\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1fd9292b-17ff-462b-8572-e640c8377fc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "      <td>['tv', 'future', 'hand', 'viewer', 'home', 'th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>['worldcom', 'bos', 'leave', 'book', 'alone', ...</td>\n",
       "      <td>worldcom bos leave book alone former worldcom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "      <td>['tiger', 'wary', 'farrell', 'gamble', 'leices...</td>\n",
       "      <td>tiger wary farrell gamble leicester say rush m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "      <td>['ocean', 'twelve', 'raid', 'box', 'office', '...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       category  \\\n",
       "0           0           tech   \n",
       "1           1       business   \n",
       "2           2          sport   \n",
       "3           3          sport   \n",
       "4           4  entertainment   \n",
       "\n",
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [tv, future, hands, viewers, home, theatre, sy...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...   \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...   \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['tv', 'future', 'hand', 'viewer', 'home', 'th...   \n",
       "1  ['worldcom', 'bos', 'leave', 'book', 'alone', ...   \n",
       "2  ['tiger', 'wary', 'farrell', 'gamble', 'leices...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raid', 'box', 'office', '...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos leave book alone former worldcom ...  \n",
       "2  tiger wary farrell gamble leicester say rush m...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# تحميل قائمة الكلمات الشائعة باللغة الإنجليزية\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# دالة لتوكنة النصوص وإزالة الكلمات الشائعة\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not isinstance(text, str):  # تحقق من أن المدخل هو نص\n",
    "        return []\n",
    "    \n",
    "    # توكنة النص\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # إزالة الكلمات الشائعة\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# تطبيق دالة توكنة النصوص وإزالة الكلمات الشائعة على عمود 'cleaned_text'\n",
    "data['tokens'] = data['cleaned_text'].apply(tokenize_and_remove_stopwords)\n",
    "# عرض أول خمس صفوف من DataFrame بعد معالجة النصوص\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3a096dfd-08a1-43dc-bf01-399e3e849b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "      <td>[tv, future, hand, viewer, home, theatre, syst...</td>\n",
       "      <td>[ ' t v ' ,   ' f u t u r e ' ,   ' h a n d ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, bos, leave, book, alone, former, wo...</td>\n",
       "      <td>[ ' w o r l d c o m ' ,   ' b o s ' ,   ' l e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "      <td>[tiger, wary, farrell, gamble, leicester, say,...</td>\n",
       "      <td>[ ' t i g e r ' ,   ' w a r y ' ,   ' f a r r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "      <td>[ ' y e a d i n g ' ,   ' f a c e ' ,   ' n e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "      <td>[ocean, twelve, raid, box, office, ocean, twel...</td>\n",
       "      <td>[ ' o c e a n ' ,   ' t w e l v e ' ,   ' r a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       category  \\\n",
       "0           0           tech   \n",
       "1           1       business   \n",
       "2           2          sport   \n",
       "3           3          sport   \n",
       "4           4  entertainment   \n",
       "\n",
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [tv, future, hands, viewers, home, theatre, sy...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...   \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...   \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  [tv, future, hand, viewer, home, theatre, syst...   \n",
       "1  [worldcom, bos, leave, book, alone, former, wo...   \n",
       "2  [tiger, wary, farrell, gamble, leicester, say,...   \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...   \n",
       "4  [ocean, twelve, raid, box, office, ocean, twel...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [ ' t v ' ,   ' f u t u r e ' ,   ' h a n d ' ...  \n",
       "1  [ ' w o r l d c o m ' ,   ' b o s ' ,   ' l e ...  \n",
       "2  [ ' t i g e r ' ,   ' w a r y ' ,   ' f a r r ...  \n",
       "3  [ ' y e a d i n g ' ,   ' f a c e ' ,   ' n e ...  \n",
       "4  [ ' o c e a n ' ,   ' t w e l v e ' ,   ' r a ...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# تحميل wordnet وملفات التصنيف النحوي\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# تهيئة WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "# دالة لتحويل التصنيف النحوي من NLTK إلى WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # الافتراضي هو الاسم\n",
    "\n",
    "# دالة التجذير (lemmatizing) مع التصنيف النحوي\n",
    "def lemmatizing(tokens):\n",
    "    # تصنيف الكلمات النحوي\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # تطبيق التجذير باستخدام التصنيف النحوي\n",
    "    return [wn.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "# تطبيق دالة lemmatizing على عمود 'tokens'\n",
    "data['lemmatized'] = data['tokens'].apply(lemmatizing)\n",
    "\n",
    "# عرض أول خمس صفوف للتحقق من النتائج\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ef04c22-0c7f-42a1-9610-1cc1021b1a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['lemmatized_text'] = data['lemmatized'].apply(lambda x: ' '.join(x)) #To convert the texts in the “lemmatized” column into single texts by joining the words using spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f1f2019-26d7-406b-b520-7f9f914b53f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aaa  aaas  aaron  abandon  abandonment  abbott  abc  abide  ability  \\\n",
      "0       0     0      0        0            0       0    0      1        0   \n",
      "1       0     0      0        0            0       0    0      0        1   \n",
      "2       0     0      0        0            0       0    0      0        0   \n",
      "3       0     0      0        0            0       0    0      0        0   \n",
      "4       0     0      0        0            0       0    0      0        0   \n",
      "...   ...   ...    ...      ...          ...     ...  ...    ...      ...   \n",
      "2220    0     0      0        0            0       0    0      0        0   \n",
      "2221    0     0      0        0            0       0    0      0        0   \n",
      "2222    0     0      0        0            0       0    0      0        0   \n",
      "2223    0     0      0        0            0       0    0      0        0   \n",
      "2224    0     0      0        0            0       0    0      0        0   \n",
      "\n",
      "      able  ...  zach  zealand  zeppelin  zero  zhang  zimbabwe  zombie  zone  \\\n",
      "0        0  ...     0        0         0     0      0         0       0     0   \n",
      "1        0  ...     0        0         0     0      0         0       0     0   \n",
      "2        0  ...     0        0         0     0      0         0       0     0   \n",
      "3        0  ...     0        0         0     0      0         0       0     0   \n",
      "4        0  ...     0        0         0     0      0         0       0     0   \n",
      "...    ...  ...   ...      ...       ...   ...    ...       ...     ...   ...   \n",
      "2220     0  ...     0        0         0     0      0         0       0     0   \n",
      "2221     0  ...     0        0         0     0      0         0       0     0   \n",
      "2222     0  ...     0        0         0     0      0         0       0     0   \n",
      "2223     0  ...     0        0         0     0      0         0       0     0   \n",
      "2224     0  ...     0        0         0     0      0         0       0     0   \n",
      "\n",
      "      zoom  zurich  \n",
      "0        0       0  \n",
      "1        0       0  \n",
      "2        0       0  \n",
      "3        0       0  \n",
      "4        0       0  \n",
      "...    ...     ...  \n",
      "2220     0       0  \n",
      "2221     0       0  \n",
      "2222     0       0  \n",
      "2223     0       0  \n",
      "2224     0       0  \n",
      "\n",
      "[2225 rows x 7085 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # تأكد من استيراد CountVectorizer\n",
    "\n",
    "#Bag of Words (BoW)\n",
    "bow_vectorizer = CountVectorizer(min_df=5)  # يمكنك تجربة قيم أعلى حسب حجم البيانات\n",
    "bow_matrix = bow_vectorizer.fit_transform(data['lemmatized_text'])\n",
    "bow_df = pd.DataFrame.sparse.from_spmatrix(bow_matrix, columns=bow_vectorizer.get_feature_names_out())\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9b8c4b8-3ae7-4c73-879e-cec4be8cf37e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "  (0, 13769)\t0.04129311624482706\n",
      "  (0, 8035)\t0.02638517393687256\n",
      "  (0, 10226)\t0.022999743917294615\n",
      "  (0, 16949)\t0.032854281535374945\n",
      "  (0, 13481)\t0.02973975872958771\n",
      "  (0, 15230)\t0.02986266308566507\n",
      "  (0, 22679)\t0.04129311624482706\n",
      "  (0, 20828)\t0.06156466137781545\n",
      "  (0, 2983)\t0.01944992347518207\n",
      "  (0, 15124)\t0.039801734316358375\n",
      "  (0, 19348)\t0.03159221091555085\n",
      "  (0, 11339)\t0.04512604622002326\n",
      "  (0, 15961)\t0.02992486065933649\n",
      "  (0, 865)\t0.024538351182000186\n",
      "  (0, 8313)\t0.042163634491354914\n",
      "  (0, 2100)\t0.026831182800885714\n",
      "  (0, 3550)\t0.02161195267797773\n",
      "  (0, 13248)\t0.03267353535191786\n",
      "  (0, 17429)\t0.054526303886417625\n",
      "  (0, 8150)\t0.039801734316358375\n",
      "  (0, 17205)\t0.044686991958911224\n",
      "  (0, 10481)\t0.046087469176965504\n",
      "  (0, 6986)\t0.021021571111838666\n",
      "  (0, 5659)\t0.06156466137781545\n",
      "  (0, 6237)\t0.03610721093939238\n",
      "  :\t:\n",
      "  (2224, 3837)\t0.06437510095714637\n",
      "  (2224, 14117)\t0.04394621653051948\n",
      "  (2224, 8625)\t0.03387863465842343\n",
      "  (2224, 8814)\t0.05126497185206189\n",
      "  (2224, 11727)\t0.03428846813940096\n",
      "  (2224, 11014)\t0.05953801414836258\n",
      "  (2224, 5149)\t0.05509036679517413\n",
      "  (2224, 12560)\t0.029720133974972536\n",
      "  (2224, 14083)\t0.05985552370084616\n",
      "  (2224, 2439)\t0.07082816572461488\n",
      "  (2224, 7009)\t0.07316193510316432\n",
      "  (2224, 6653)\t0.045229716531085776\n",
      "  (2224, 15356)\t0.08484667409704565\n",
      "  (2224, 8453)\t0.11040764551695709\n",
      "  (2224, 232)\t0.04053413917929468\n",
      "  (2224, 18067)\t0.06162736030327651\n",
      "  (2224, 22494)\t0.08029840421247239\n",
      "  (2224, 630)\t0.028559193238865525\n",
      "  (2224, 13736)\t0.0470417487034063\n",
      "  (2224, 22324)\t0.08395844770537154\n",
      "  (2224, 15670)\t0.043548603592374606\n",
      "  (2224, 16861)\t0.10323698861156969\n",
      "  (2224, 22988)\t0.05201279582623971\n",
      "  (2224, 13694)\t0.04720119372650462\n",
      "  (2224, 8191)\t0.05617018676134458\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#TF-IDF\n",
    "tfidf_BBC = TfidfVectorizer(min_df=1)#Ignore rare words\n",
    "tfidf_matrix = tfidf_BBC.fit_transform(data['lemmatized_text'])#tfidf_matrix-Represents the weight of each word based on its frequency.\n",
    "#Fit_transform-To convert texts to a matrix.\n",
    "\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix)\n",
    "with open(\"tfidf_BBC.pickle\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_BBC, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f743d35-fc4c-407e-b4ae-b466f82a798b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9685393258426966\n",
      "SVM Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.92      0.94       101\n",
      "entertainment       0.98      0.99      0.98        81\n",
      "     politics       0.94      0.96      0.95        83\n",
      "        sport       0.98      1.00      0.99        98\n",
      "         tech       0.99      0.98      0.98        82\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n",
      "Naive Bayes Accuracy: 0.9573033707865168\n",
      "Naive Bayes Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.94      0.94       101\n",
      "entertainment       1.00      0.89      0.94        81\n",
      "     politics       0.92      0.98      0.95        83\n",
      "        sport       0.99      1.00      0.99        98\n",
      "         tech       0.94      0.98      0.96        82\n",
      "\n",
      "     accuracy                           0.96       445\n",
      "    macro avg       0.96      0.96      0.96       445\n",
      " weighted avg       0.96      0.96      0.96       445\n",
      "\n",
      "Random Forest Accuracy: 0.946067415730337\n",
      "Random Forest Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.90      0.95      0.92       101\n",
      "entertainment       0.99      0.90      0.94        81\n",
      "     politics       0.94      0.94      0.94        83\n",
      "        sport       0.98      0.99      0.98        98\n",
      "         tech       0.94      0.94      0.94        82\n",
      "\n",
      "     accuracy                           0.95       445\n",
      "    macro avg       0.95      0.94      0.95       445\n",
      " weighted avg       0.95      0.95      0.95       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import pickle\n",
    "\n",
    "# إعداد البيانات للتقسيم\n",
    "X = tfidf_matrix\n",
    "y = data['category'] \n",
    "\n",
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)  #Split data\n",
    "\n",
    "#Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)  #Make predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)  #Calculate accuracy\n",
    "    report = classification_report(y_test, predictions)  #Generate classification report\n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear', class_weight='balanced')  #Initialize SVM model with linear kernel and class weights\n",
    "svm_model.fit(X_train, y_train)  #Train the model\n",
    "svm_accuracy, svm_report = evaluate_model(svm_model, X_test, y_test)  #Evaluate SVM\n",
    "print(\"SVM Accuracy:\", svm_accuracy)  #Print accuracy\n",
    "print(\"SVM Classification Report:\\n\", svm_report)  #Print classification report\n",
    "\n",
    "\n",
    "nb_model = MultinomialNB()  #Initialize Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)  #Train the model\n",
    "nb_accuracy, nb_report = evaluate_model(nb_model, X_test, y_test)  #Evaluate Naive Bayes\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)  #Print accuracy\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)  #Print classification report\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)  #Initialize Random Forest model with class weights\n",
    "rf_model.fit(X_train, y_train)  #Train the model\n",
    "rf_accuracy, rf_report = evaluate_model(rf_model, X_test, y_test)  #Evaluate Random Forest\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)  #Print accuracy\n",
    "print(\"Random Forest Classification Report:\\n\", rf_report)  #Print classification report\n",
    "with open(\"model_bbc.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(svm_model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "560389a9-81ca-4526-aad5-ac7a8e2258e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category is: tech\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# دالة معالجة البيانات\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()  # تحويل النص إلى حروف صغيرة\n",
    "    text = re.sub(r'[^\\w\\s\\'\\\"]', '', text)  # إزالة علامات الترقيم\n",
    "    return text\n",
    "\n",
    "# التأكد من عدم وجود نصوص فارغة\n",
    "data = data[data['lemmatized_text'].notnull() & (data['lemmatized_text'] != '')]\n",
    "\n",
    "# معالجة البيانات\n",
    "data['lemmatized_text'] = data['lemmatized_text'].apply(preprocess_data)\n",
    "\n",
    "# إنشاء مصفوفة TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "# إعداد البيانات\n",
    "X = tfidf_matrix\n",
    "y = data['category']\n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# تدريب نموذج الانحدار اللوجستي\n",
    "lg_model = LogisticRegression(max_iter=200, verbose=1)\n",
    "lg_model.fit(X_train, y_train)\n",
    "\n",
    "# حفظ النموذج وtfidf_vectorizer\n",
    "with open(\"model_bbc.pickle\", \"rb\") as model_file:\n",
    "    svm_model = pickle.load(model_file)\n",
    "\n",
    "with open(\"tfidf_BBC.pickle\", \"rb\") as file:\n",
    "    tfidf_BBC = pickle.load(file)\n",
    "\n",
    "# دالة التنبؤ\n",
    "def predict_category(text):\n",
    "    cleaned_text = preprocess_data(text)  # معالجة النص\n",
    "    tfidf_data = tfidf_vectorizer.transform([cleaned_text])  # تحويل النص إلى تمثيل TF-IDF\n",
    "    prediction = lg_model.predict(tfidf_data)  # إجراء التنبؤ\n",
    "    return prediction[0]  # إرجاع الفئة المتوقعة\n",
    "\n",
    "\n",
    "# اختبار دالة التنبؤ\n",
    "new_text = \"video games \"\n",
    "predicted_category = predict_category(new_text)\n",
    "print(f\"The predicted category is: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "61bb2e6c-a54b-4c8b-98ea-ca1789104713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>رقم  التصنيف</th>\n",
       "      <th>صنف الخبر</th>\n",
       "      <th>عنوان الخبر</th>\n",
       "      <th>clean</th>\n",
       "      <th>cleaned_stopwords</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>صحة</td>\n",
       "      <td>الجلوس أكثر من 10 ساعات يومياً يفاقم مخاطر الوفاة</td>\n",
       "      <td>الجلوس اكثر من  ساعات يومياً يفاقم مخاطر الوفاه</td>\n",
       "      <td>الجلوس اكثر ساعات يوميا يفاقم مخاطر الوفاه</td>\n",
       "      <td>[('جلوس', 'noun'), ('كثر', 'verb'), ('ساعة', '...</td>\n",
       "      <td>جلوس كثر ساعة يوم فاقم مخاطر الوفاه</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>رياضة</td>\n",
       "      <td>مولر: هناك طريقة واحدة لإيقاف خطورة ميسي</td>\n",
       "      <td>مولر هناك طريقه واحده لايقاف خطوره ميسي</td>\n",
       "      <td>مولر طريقه واحده لايقاف خطوره ميسي</td>\n",
       "      <td>[('مولر', 'all'), ('طريق', 'noun'), ('واحد', '...</td>\n",
       "      <td>مولر طريق واحد لايقاف خطوره ميس</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>اقتصاد</td>\n",
       "      <td>وزير مالية لبنان: \"إجراءات تقشفية استثنائية\" ب...</td>\n",
       "      <td>وزير ماليه لبنان اجراءات تقشفيه استثنائيه بموا...</td>\n",
       "      <td>وزير ماليه لبنان اجراءات تقشفيه استثنائيه بموازنه</td>\n",
       "      <td>[('زير', 'noun'), ('مال', 'noun'), ('لبن', 'no...</td>\n",
       "      <td>زير مال لبن اجراءات تقشف استثناء موازن</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>فن</td>\n",
       "      <td>مروان خوري: هكذا أقضي فترة الحجر المنزلي</td>\n",
       "      <td>مروان خوري هكذا اقضي فتره الحجر المنزلي</td>\n",
       "      <td>مروان خوري اقضي فتره الحجر المنزلي</td>\n",
       "      <td>[('مرو', 'noun'), ('خور', 'noun'), ('قضى', 've...</td>\n",
       "      <td>مرو خور قضى فتر حجر منزل</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>اقتصاد</td>\n",
       "      <td>مصر تبدأ حملة التطعيم بلقاح كورونا لعامة الشعب</td>\n",
       "      <td>مصر تبدا حمله التطعيم بلقاح كورونا لعامه الشعب</td>\n",
       "      <td>مصر تبدا حمله التطعيم بلقاح كورونا لعامه الشعب</td>\n",
       "      <td>[('مصر', 'noun'), ('بد', 'verb'), ('حمل', 'nou...</td>\n",
       "      <td>مصر بد حمل تطعيم لقاح كار عام شعب</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   رقم  التصنيف صنف الخبر                                        عنوان الخبر  \\\n",
       "0             0       صحة  الجلوس أكثر من 10 ساعات يومياً يفاقم مخاطر الوفاة   \n",
       "1             5     رياضة           مولر: هناك طريقة واحدة لإيقاف خطورة ميسي   \n",
       "2             6    اقتصاد  وزير مالية لبنان: \"إجراءات تقشفية استثنائية\" ب...   \n",
       "3             4        فن           مروان خوري: هكذا أقضي فترة الحجر المنزلي   \n",
       "4             6    اقتصاد     مصر تبدأ حملة التطعيم بلقاح كورونا لعامة الشعب   \n",
       "\n",
       "                                               clean  \\\n",
       "0    الجلوس اكثر من  ساعات يومياً يفاقم مخاطر الوفاه   \n",
       "1            مولر هناك طريقه واحده لايقاف خطوره ميسي   \n",
       "2  وزير ماليه لبنان اجراءات تقشفيه استثنائيه بموا...   \n",
       "3            مروان خوري هكذا اقضي فتره الحجر المنزلي   \n",
       "4     مصر تبدا حمله التطعيم بلقاح كورونا لعامه الشعب   \n",
       "\n",
       "                                   cleaned_stopwords  \\\n",
       "0         الجلوس اكثر ساعات يوميا يفاقم مخاطر الوفاه   \n",
       "1                 مولر طريقه واحده لايقاف خطوره ميسي   \n",
       "2  وزير ماليه لبنان اجراءات تقشفيه استثنائيه بموازنه   \n",
       "3                 مروان خوري اقضي فتره الحجر المنزلي   \n",
       "4     مصر تبدا حمله التطعيم بلقاح كورونا لعامه الشعب   \n",
       "\n",
       "                                     lemmatized_text  \\\n",
       "0  [('جلوس', 'noun'), ('كثر', 'verb'), ('ساعة', '...   \n",
       "1  [('مولر', 'all'), ('طريق', 'noun'), ('واحد', '...   \n",
       "2  [('زير', 'noun'), ('مال', 'noun'), ('لبن', 'no...   \n",
       "3  [('مرو', 'noun'), ('خور', 'noun'), ('قضى', 've...   \n",
       "4  [('مصر', 'noun'), ('بد', 'verb'), ('حمل', 'nou...   \n",
       "\n",
       "                               lemmatized  \n",
       "0     جلوس كثر ساعة يوم فاقم مخاطر الوفاه  \n",
       "1         مولر طريق واحد لايقاف خطوره ميس  \n",
       "2  زير مال لبن اجراءات تقشف استثناء موازن  \n",
       "3                مرو خور قضى فتر حجر منزل  \n",
       "4       مصر بد حمل تطعيم لقاح كار عام شعب  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = pd.read_excel(r'C:\\Users\\s7r_2\\Downloads\\��تصنيف الاخبار العربيه - نسخة.xlsx')\n",
    "text.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "860e68f1-069a-4867-8b6c-ad24331c5e56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words (BoW) Matrix:\n",
      "  (0, 3521)\t1\n",
      "  (0, 5882)\t1\n",
      "  (0, 4452)\t1\n",
      "  (0, 7947)\t1\n",
      "  (0, 5504)\t1\n",
      "  (0, 6609)\t1\n",
      "  (0, 1935)\t1\n",
      "  (1, 7241)\t1\n",
      "  (1, 5063)\t1\n",
      "  (1, 7640)\t1\n",
      "  (1, 6117)\t1\n",
      "  (1, 3914)\t1\n",
      "  (1, 7265)\t1\n",
      "  (2, 4429)\t1\n",
      "  (2, 6406)\t1\n",
      "  (2, 6129)\t1\n",
      "  (2, 299)\t1\n",
      "  (2, 3172)\t1\n",
      "  (2, 496)\t1\n",
      "  (2, 7184)\t1\n",
      "  (3, 6717)\t1\n",
      "  (3, 3953)\t1\n",
      "  (3, 5755)\t1\n",
      "  (3, 5521)\t1\n",
      "  (3, 3653)\t1\n",
      "  :\t:\n",
      "  (57783, 1114)\t1\n",
      "  (57783, 4973)\t1\n",
      "  (57783, 4528)\t1\n",
      "  (57783, 7894)\t1\n",
      "  (57783, 5389)\t1\n",
      "  (57784, 5832)\t1\n",
      "  (57784, 4489)\t1\n",
      "  (57784, 820)\t1\n",
      "  (57784, 2716)\t1\n",
      "  (57784, 4423)\t1\n",
      "  (57784, 7885)\t1\n",
      "  (57784, 2260)\t1\n",
      "  (57785, 6864)\t1\n",
      "  (57785, 3630)\t1\n",
      "  (57785, 4663)\t1\n",
      "  (57785, 2211)\t1\n",
      "  (57785, 7835)\t1\n",
      "  (57785, 4391)\t1\n",
      "  (57786, 7083)\t1\n",
      "  (57786, 4077)\t1\n",
      "  (57786, 4029)\t1\n",
      "  (57786, 2615)\t1\n",
      "  (57786, 3780)\t1\n",
      "  (57786, 4640)\t1\n",
      "  (57786, 3107)\t1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # تأكد من استيراد CountVectorizer\n",
    "text = pd.read_excel(r'C:\\Users\\s7r_2\\Downloads\\��تصنيف الاخبار العربيه - نسخة.xlsx')\n",
    "#Bag of Words (BoW)\n",
    "if 'lemmatized' in text.columns:\n",
    "    # حساب Bag-of-Words\n",
    "    bow_vectorizer = CountVectorizer(min_df=5, max_features=10000)  # تجاهل الكلمات النادرة\n",
    "    bow_matrix = bow_vectorizer.fit_transform(text['lemmatized'])\n",
    "\n",
    "#Convert to DataFrame for better visualization\n",
    "print(\"Bag of Words (BoW) Matrix:\")\n",
    "print(bow_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3698f4f8-cf88-40fb-87d8-a881f622397e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "  (0, 6561)\t0.40070590003235035\n",
      "  (0, 19245)\t0.3569809072021846\n",
      "  (0, 15645)\t0.4787266780801195\n",
      "  (0, 23502)\t0.2757473843498377\n",
      "  (0, 13349)\t0.34733696028046385\n",
      "  (0, 16561)\t0.2875553569645421\n",
      "  (0, 11274)\t0.4510571072437371\n",
      "  (1, 20755)\t0.3232238638689826\n",
      "  (1, 12022)\t0.451721502828749\n",
      "  (1, 17490)\t0.49959939467166087\n",
      "  (1, 21749)\t0.38327049338141533\n",
      "  (1, 14713)\t0.32706039214003524\n",
      "  (1, 20669)\t0.43360084334307375\n",
      "  (2, 20538)\t0.4196464836883769\n",
      "  (2, 1967)\t0.4095251924467677\n",
      "  (2, 10536)\t0.48373853470560607\n",
      "  (2, 1327)\t0.3743634345025407\n",
      "  (2, 17528)\t0.30043158842465145\n",
      "  (2, 18729)\t0.32171591716558123\n",
      "  (2, 13245)\t0.2971106254355494\n",
      "  (3, 20412)\t0.35712435440574086\n",
      "  (3, 11561)\t0.38540036847408293\n",
      "  (3, 15694)\t0.3958948980251058\n",
      "  (3, 16231)\t0.3946192916142427\n",
      "  (3, 12095)\t0.4428547920110705\n",
      "  :\t:\n",
      "  (57783, 3671)\t0.27795453392227626\n",
      "  (57783, 10066)\t0.24074337432942144\n",
      "  (57783, 16318)\t0.2895854845623391\n",
      "  (57783, 11748)\t0.3279545476294673\n",
      "  (57784, 7637)\t0.5674403180275363\n",
      "  (57784, 23311)\t0.3799228536890273\n",
      "  (57784, 13236)\t0.4281325455287011\n",
      "  (57784, 9609)\t0.4127550056733554\n",
      "  (57784, 2939)\t0.21736249114626413\n",
      "  (57784, 13456)\t0.27989212559829696\n",
      "  (57784, 16387)\t0.23328069989404357\n",
      "  (57785, 10879)\t0.5508083430233048\n",
      "  (57785, 13158)\t0.49712154351202054\n",
      "  (57785, 23039)\t0.33759317737014066\n",
      "  (57785, 7417)\t0.3799416028836\n",
      "  (57785, 13866)\t0.2683914181752422\n",
      "  (57785, 11519)\t0.28506120311230965\n",
      "  (57785, 19768)\t0.19458088897311127\n",
      "  (57786, 10413)\t0.45934612066718306\n",
      "  (57786, 13816)\t0.4119397502372923\n",
      "  (57786, 11767)\t0.36863620446031004\n",
      "  (57786, 9258)\t0.42784234317722697\n",
      "  (57786, 12295)\t0.32987126544405904\n",
      "  (57786, 12408)\t0.34343438233226464\n",
      "  (57786, 20290)\t0.2712984242758415\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "text = pd.read_excel(r'C:\\Users\\s7r_2\\Downloads\\��تصنيف الاخبار العربيه - نسخة.xlsx')\n",
    "#TF-IDF\n",
    "tfidf_Arabic = TfidfVectorizer(min_df=1)#Ignore rare words\n",
    "tfidf_matrix = tfidf_Arabic.fit_transform(text['lemmatized'])#tfidf_matrix-Represents the weight of each word based on its frequency.\n",
    "#Fit_transform-To convert texts to a matrix.\n",
    "\n",
    "#Convert to DataFrame to display the data more clearly\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_matrix)\n",
    "with open(\"tfidf_Arabic.pkl\", \"wb\") as tfidf_file:\n",
    "    pickle.dump(tfidf_Arabic,tfidf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "744cdf4f-85f2-4a9a-a667-d3d72d5e55b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9103651150718117\n",
      "SVM Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      اقتصاد       0.89      0.89      0.89      2113\n",
      " التكنولوجيا       0.92      0.91      0.92      1499\n",
      "     السياحة       0.90      0.91      0.91      1029\n",
      "       رياضة       0.98      0.96      0.97      1956\n",
      "       سياسة       0.85      0.85      0.85      1481\n",
      "         صحة       0.89      0.93      0.91      1511\n",
      "          فن       0.92      0.91      0.91      1969\n",
      "\n",
      "    accuracy                           0.91     11558\n",
      "   macro avg       0.91      0.91      0.91     11558\n",
      "weighted avg       0.91      0.91      0.91     11558\n",
      "\n",
      "Naive Bayes Accuracy: 0.8857068697006403\n",
      "Naive Bayes Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      اقتصاد       0.75      0.95      0.84      2113\n",
      " التكنولوجيا       0.94      0.87      0.90      1499\n",
      "     السياحة       0.95      0.71      0.81      1029\n",
      "       رياضة       0.96      0.96      0.96      1956\n",
      "       سياسة       0.90      0.76      0.82      1481\n",
      "         صحة       0.91      0.91      0.91      1511\n",
      "          فن       0.90      0.93      0.91      1969\n",
      "\n",
      "    accuracy                           0.89     11558\n",
      "   macro avg       0.90      0.87      0.88     11558\n",
      "weighted avg       0.89      0.89      0.89     11558\n",
      "\n",
      "Random Forest Accuracy: 0.850147084270635\n",
      "Random Forest Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "      اقتصاد       0.84      0.85      0.84      2113\n",
      " التكنولوجيا       0.90      0.86      0.88      1499\n",
      "     السياحة       0.88      0.86      0.87      1029\n",
      "       رياضة       0.92      0.92      0.92      1956\n",
      "       سياسة       0.81      0.74      0.77      1481\n",
      "         صحة       0.81      0.86      0.84      1511\n",
      "          فن       0.80      0.84      0.82      1969\n",
      "\n",
      "    accuracy                           0.85     11558\n",
      "   macro avg       0.85      0.85      0.85     11558\n",
      "weighted avg       0.85      0.85      0.85     11558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "\n",
    "# تحميل مجموعة البيانات\n",
    "\n",
    "# تحويل النصوص إلى ميزات عددية باستخدام TF-IDF\n",
    "tfidf_Arabic = TfidfVectorizer(min_df=1)\n",
    "X = tfidf_Arabic.fit_transform(text['lemmatized'])  # تحويل النصوص إلى مصفوفة TF-IDF\n",
    "\n",
    "# إعداد البيانات للتقسيم\n",
    "y = text['صنف الخبر'] \n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# دالة لتقييم أداء النموذج\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)  # توقع النتائج\n",
    "    accuracy = accuracy_score(y_test, predictions)  # حساب الدقة\n",
    "    report = classification_report(y_test, predictions)  # توليد تقرير التصنيف\n",
    "    return accuracy, report\n",
    "\n",
    "# نموذج SVM\n",
    "svm_model_ar = SVC(kernel='linear', class_weight='balanced')  # تهيئة نموذج SVM\n",
    "svm_model_ar.fit(X_train, y_train)  # تدريب النموذج\n",
    "svm_accuracy, svm_report = evaluate_model(svm_model_ar, X_test, y_test)  # تقييم SVM\n",
    "print(\"SVM Accuracy:\", svm_accuracy)  # طباعة الدقة\n",
    "print(\"SVM Classification Report:\\n\", svm_report)  # طباعة تقرير التصنيف\n",
    "\n",
    "# نموذج Naive Bayes\n",
    "nb_model = MultinomialNB()  # تهيئة نموذج Naive Bayes\n",
    "nb_model.fit(X_train, y_train)  # تدريب النموذج\n",
    "nb_accuracy, nb_report = evaluate_model(nb_model, X_test, y_test)  # تقييم Naive Bayes\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)  # طباعة الدقة\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)  # طباعة تقرير التصنيف\n",
    "\n",
    "# نموذج Random Forest\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)  # تهيئة نموذج Random Forest\n",
    "rf_model.fit(X_train, y_train)  # تدريب النموذج\n",
    "rf_accuracy, rf_report = evaluate_model(rf_model, X_test, y_test)  # تقييم Random Forest\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)  # طباعة الدقة\n",
    "print(\"Random Forest Classification Report:\\n\", rf_report)  # طباعة تقرير التصنيف\n",
    "\n",
    "# حفظ نموذج SVM\n",
    "with open(\"model_Arabic.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(svm_model_ar, model_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1a043ae-c829-488c-9552-dcf84b6e088e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category is: اقتصاد\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import re\n",
    "\n",
    "# دالة معالجة البيانات\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()  # تحويل النص إلى حروف صغيرة\n",
    "    text = re.sub(r'[^\\w\\s\\'\\\"]', '', text)  # إزالة علامات الترقيم\n",
    "    return text\n",
    "\n",
    "# تحميل البيانات\n",
    "text = text[text['lemmatized'].notnull() & (text['lemmatized'] != '')]\n",
    "\n",
    "# معالجة البيانات\n",
    "text['lemmatized'] = text['lemmatized'].apply(preprocess_data)\n",
    "\n",
    "# إنشاء مصفوفة TF-IDF\n",
    "tfidf_vectorizer = TfidfVectorizer(min_df=5)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(text['lemmatized'])\n",
    "\n",
    "# إعداد البيانات\n",
    "X = tfidf_matrix\n",
    "y = text['صنف الخبر']\n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# تدريب نموذج الانحدار اللوجستي\n",
    "lg_model = LogisticRegression(max_iter=200, verbose=1)\n",
    "lg_model.fit(X_train, y_train)\n",
    "\n",
    "# تخزين النموذج وTF-IDF\n",
    "with open(\"model_Arabic.pickle\", \"rb\") as model_file:\n",
    "    svm_model_ar = pickle.load(model_file)\n",
    "\n",
    "with open(\"tfidf_Arabic.pkl\", \"rb\") as tfidf_file:\n",
    "    tfidf_Arabic = pickle.load(tfidf_file)\n",
    "\n",
    "# دالة التنبؤ\n",
    "def predict_category(text):\n",
    "    cleaned_text = preprocess_data(text)  # معالجة النص\n",
    "    tfidf_data = tfidf_vectorizer.transform([cleaned_text])  # تحويل النص إلى تمثيل TF-IDF\n",
    "    prediction = lg_model.predict(tfidf_data)  # إجراء التنبؤ\n",
    "    return prediction[0]  # إرجاع الفئة المتوقعة\n",
    "\n",
    "# نص جديد للتنبؤ بفئته\n",
    "new_text = \"وزير مالية لبنان:إجراءات تقشفية استثنائية بموازنة 2019\"\n",
    "predicted_category = predict_category(new_text)\n",
    "print(f\"The predicted category is: {predicted_category}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ede20fd4-2800-420d-b1d8-d0fd7e7f802d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "import joblib  # لتحميل النموذج\n",
    "\n",
    "# تحميل النموذجين BBC و Arabic\n",
    "#BBC\n",
    "with open(\"model_bbc.pickle\", \"rb\") as model_file:\n",
    "    svm_model = pickle.load(model_file)\n",
    "with open(\"tfidf_BBC.pickle\", \"rb\") as file:\n",
    "    tfidf_BBC = pickle.load(file)\n",
    "    \n",
    "#Arabic\n",
    "with open(\"tfidf_Arabic.pkl\", \"rb\") as tfidf_file:\n",
    "    tfidf_Arabic = pickle.load(tfidf_file)\n",
    "with open(\"model_Arabic.pickle\", \"rb\") as model_file:\n",
    "    svm_model_ar = pickle.load(model_file)\n",
    "\n",
    "# المتغير لتحديد اللغة الحالية\n",
    "current_language = \"english\"  # البداية باللغة الإنجليزية\n",
    "\n",
    "# دالة التصنيف باستخدام النموذج المحمل\n",
    "# دالة التصنيف باستخدام النموذج المحمل\n",
    "def predict_category(text):\n",
    "    if current_language == \"english\":\n",
    "        text_features = tfidf_BBC.transform([text])\n",
    "        prediction = svm_model.predict(text_features)\n",
    "        category_map = {0: 'tech', 1: 'business', 2: 'sport', 3: 'entertainment', 4: 'politics'}\n",
    "    else:\n",
    "        text_features = tfidf_Arabic.transform([text])\n",
    "        prediction = svm_model_ar.predict(text_features)\n",
    "        category_map = {\n",
    "            0: 'صحة',\n",
    "            1: 'سياحي',\n",
    "            2: 'سياسي', \n",
    "            3: 'تكنولوجيا',\n",
    "            4: 'اقتصادي',\n",
    "            5: 'رياضي'\n",
    "        }\n",
    "\n",
    "def predict_category(text):\n",
    "    if current_language == \"english\":\n",
    "        cleaned_text = preprocess_data(text)  # معالجة النص\n",
    "        tfidf_data = tfidf_BBC.transform([cleaned_text])  # تحويل النص إلى تمثيل TF-IDF\n",
    "        prediction = svm_model.predict(tfidf_data)  # إجراء التنبؤ\n",
    "        category_map = {0: 'tech', 1: 'business', 2: 'sport', 3: 'entertainment', 4: 'politics'}\n",
    "        return category_map.get(prediction[0])  # إرجاع الفئة المتوقعة\n",
    "    else:\n",
    "        cleaned_text = preprocess_data(text)  # معالجة النص\n",
    "        tfidf_data = tfidf_Arabic.transform([cleaned_text])  # تحويل النص إلى تمثيل TF-IDF\n",
    "        prediction = svm_model_ar.predict(tfidf_data)  # إجراء التنبؤ\n",
    "        category_map = {\n",
    "            0: 'صحة',\n",
    "            1: 'سياحي',\n",
    "            2: 'سياسي', \n",
    "            3: 'تكنولوجيا',\n",
    "            4: 'اقتصادي',\n",
    "            5: 'رياضي',\n",
    "            6: 'فن'\n",
    "        }\n",
    "        return category_map.get(prediction[0])  # إرجاع الفئة المتوقعة\n",
    "     \n",
    "\n",
    "# إعداد دالة لتصنيف النص\n",
    "def classify_text():\n",
    "    text = entry.get()\n",
    "    if not text.strip():\n",
    "        messagebox.showwarning(\"خطأ\", \"الرجاء إدخال نص للتصنيف!\")\n",
    "        return\n",
    "    \n",
    "    # احصل على التصنيف\n",
    "    category = predict_category(text)\n",
    "    \n",
    "    # تحديث النتيجة في واجهة tkinter\n",
    "    result_label.config(text=f\"نوع الخبر: {category}\")\n",
    "    result_label.update()  # لضمان التحديث الفوري في الواجهة\n",
    "\n",
    "# إعداد دالة لمسح النص\n",
    "def clear_text():\n",
    "    entry.delete(0, tk.END)\n",
    "    result_label.config(text=\"\")\n",
    "\n",
    "# دالة لتغيير اللغة إلى العربية\n",
    "def set_language_arabic():\n",
    "    global current_language\n",
    "    current_language = \"arabic\"\n",
    "    entry_label.config(text=\"أدخل النص:\")\n",
    "    classify_button.config(text=\"تصنيف\")\n",
    "    clear_button.config(text=\"حذف النص\")\n",
    "    result_label.config(text=\"\")\n",
    "    lang_frame.pack_forget()  # إخفاء شاشة اختيار اللغة\n",
    "    main_frame.pack(fill=\"both\", expand=True)  # عرض شاشة التصنيف\n",
    "\n",
    "def set_language_english():\n",
    "    global current_language\n",
    "    current_language = \"english\"\n",
    "    entry_label.config(text=\"Enter text:\")\n",
    "    classify_button.config(text=\"Classify\")\n",
    "    clear_button.config(text=\"Clear Text\")\n",
    "    result_label.config(text=\"\")\n",
    "    lang_frame.pack_forget()  # إخفاء شاشة اختيار اللغة\n",
    "    main_frame.pack(fill=\"both\", expand=True)  # عرض شاشة التصنيف\n",
    "\n",
    "# إعداد النافذة الرئيسية\n",
    "root = tk.Tk()\n",
    "root.geometry(\"800x500\")\n",
    "root.title(\"News Classification\")\n",
    "\n",
    "# شاشة اختيار اللغة\n",
    "lang_frame = tk.Frame(root)\n",
    "lang_frame.pack(fill=\"both\", expand=True)\n",
    "arabic_button = tk.Button(lang_frame, text=\"عربي\", font=(\"Arial\", 16), command=set_language_arabic)\n",
    "arabic_button.pack(side=\"left\", expand=True, padx=20, pady=20)\n",
    "english_button = tk.Button(lang_frame, text=\"English\", font=(\"Arial\", 16), command=set_language_english)\n",
    "english_button.pack(side=\"right\", expand=True, padx=20, pady=20)\n",
    "\n",
    "# شاشة التصنيف الرئيسية\n",
    "main_frame = tk.Frame(root)\n",
    "\n",
    "entry_label = tk.Label(main_frame, text=\"\", font=(\"Arial\", 14))\n",
    "entry_label.pack(pady=10)\n",
    "\n",
    "entry = tk.Entry(main_frame, font=(\"Arial\", 14), width=50)\n",
    "entry.pack(pady=10)\n",
    "\n",
    "classify_button = tk.Button(main_frame, text=\"\", font=(\"Arial\", 14), command=classify_text)\n",
    "classify_button.pack(pady=10)\n",
    "\n",
    "clear_button = tk.Button(main_frame, text=\"\", font=(\"Arial\", 14), command=clear_text)\n",
    "clear_button.pack(pady=10)\n",
    "\n",
    "result_label = tk.Label(main_frame, text=\"\", font=(\"Arial\", 16), fg=\"blue\")\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "# بدء تشغيل النافذة\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e588b2fc-249f-408e-85e8-5096a2fc60d6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b65c44-d1b0-46f4-b92a-5556452a2170",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
