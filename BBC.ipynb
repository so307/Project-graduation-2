{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea5d270-1bce-4c52-a3b9-78286ca4e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing libraries\n",
    "import pandas as pd  #Data manipulation\n",
    "import string  #Remove punctuation & characters\n",
    "import nltk  #Natural language processing \n",
    "import pickle  #For loading saved models and vectorizers\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords  #Stop word removal\n",
    "from nltk.tokenize import word_tokenize  #Tokenizition\n",
    "#from nltk.stem import PorterStemmer  #Stemming\n",
    "from nltk.stem import WordNetLemmatizer  #Import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet  #Import WordNet\n",
    "\n",
    "#Feature extractions libraries\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#Models libraries\n",
    "from sklearn.model_selection import train_test_split #For data splitting\n",
    "#Model Evaluation Function\n",
    "from sklearn.metrics import accuracy_score, classification_report  #Import metrics\n",
    "from sklearn.svm import SVC  #SVM Model\n",
    "from sklearn.naive_bayes import MultinomialNB #Naive Bayes Model -  MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82144e6-6fc0-466d-a10f-35db9b8c6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab4c6eef-f705-4164-87ca-781586a896e0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#Read excel file\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124ms7r_2\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mDownloads\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mbbc-text (1).csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "#Read excel file\n",
    "data = pd.read_csv(r'C:\\Users\\s7r_2\\Downloads\\bbc-text (1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e7b91d10-bf3e-4c72-9e1e-b94b667c2c7a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 38\u001b[0m\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(tokens)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Ensure there is a 'text' column with tokens to clean\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# Apply the cleaning function to the 'text' column and save the results in a new column\u001b[39;00m\n\u001b[1;32m---> 38\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(clean_text)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Display the first five rows of the DataFrame after cleaning\u001b[39;00m\n\u001b[0;32m     41\u001b[0m data\u001b[38;5;241m.\u001b[39mhead()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "# Function to remove non-ASCII characters\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "# Function to clean text\n",
    "def clean_text(text):\n",
    "    # Ensure the input is a string\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r\"what's\", \"what is \", text)  # Replace contractions\n",
    "    text = text.replace('(ap)', '')  # Remove specific unwanted substrings\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)  # Replace non-word characters with a space\n",
    "    text = re.sub(r'\\s+', ' ', text)  # Replace multiple spaces with a single space\n",
    "    text = re.sub(r\"\\\\\", \"\", text)  # Remove backslashes\n",
    "    text = re.sub(r\"\\'\", \"\", text)  # Remove single quotes\n",
    "    text = re.sub(r\"\\\"\", \"\", text)  # Remove double quotes\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)  # Keep only letters and specific symbols\n",
    "    text = _removeNonAscii(text)  # Remove non-ASCII characters\n",
    "    text = text.strip()  # Trim leading and trailing spaces\n",
    "    return text\n",
    "\n",
    "# Function to convert a list of words into a single string\n",
    "def list_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Ensure there is a 'text' column with tokens to clean\n",
    "# Apply the cleaning function to the 'text' column and save the results in a new column\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# Display the first five rows of the DataFrame after cleaning\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d58964a-0807-4cd0-830e-646ef279f0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [tv, future, hands, viewers, home, theatre, sy...  \n",
       "1  [worldcom, boss, left, books, alone, former, w...  \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...  \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the list of common stopwords in English\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Function to tokenize text and remove stopwords\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not isinstance(text, str):  # Ensure the input is a string\n",
    "        return []\n",
    "    \n",
    "    # Tokenize the text into individual words\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # Remove stopwords from the tokens\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# Apply the tokenization and stopword removal function to the 'cleaned_text' column\n",
    "data['tokens'] = data['cleaned_text'].apply(tokenize_and_remove_stopwords)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fb8f2ba-671b-44f2-9bc4-fb37f34fddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "      <td>[tv, future, hand, viewer, home, theatre, syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, bos, leave, book, alone, former, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "      <td>[tiger, wary, farrell, gamble, leicester, say,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "      <td>[ocean, twelve, raid, box, office, ocean, twel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [tv, future, hands, viewers, home, theatre, sy...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...   \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...   \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [tv, future, hand, viewer, home, theatre, syst...  \n",
       "1  [worldcom, bos, leave, book, alone, former, wo...  \n",
       "2  [tiger, wary, farrell, gamble, leicester, say,...  \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4  [ocean, twelve, raid, box, office, ocean, twel...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Download WordNet and the POS (Part-Of-Speech) tagger data\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Initialize WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "# Function to map POS tags from NLTK to WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):  # Adjective\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):  # Verb\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):  # Noun\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):  # Adverb\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match\n",
    "\n",
    "# Lemmatizing function with POS tagging\n",
    "def lemmatizing(tokens):\n",
    "    # Get POS tags for the tokens\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # Apply lemmatization using the POS tags\n",
    "    return [wn.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "# Apply the lemmatizing function to the 'tokens' column\n",
    "data['lemmatized'] = data['tokens'].apply(lemmatizing)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7626c459-0a9d-4e63-bf42-ebbb63ec2870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>['tv', 'future', 'hands', 'viewers', 'home', '...</td>\n",
       "      <td>['tv', 'future', 'hand', 'viewer', 'home', 'th...</td>\n",
       "      <td>[ ' t v ' ,   ' f u t u r e ' ,   ' h a n d ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>['worldcom', 'boss', 'left', 'books', 'alone',...</td>\n",
       "      <td>['worldcom', 'bos', 'leave', 'book', 'alone', ...</td>\n",
       "      <td>[ ' w o r l d c o m ' ,   ' b o s ' ,   ' l e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>['tigers', 'wary', 'farrell', 'gamble', 'leice...</td>\n",
       "      <td>['tiger', 'wary', 'farrell', 'gamble', 'leices...</td>\n",
       "      <td>[ ' t i g e r ' ,   ' w a r y ' ,   ' f a r r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>[ ' y e a d i n g ' ,   ' f a c e ' ,   ' n e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>['ocean', 'twelve', 'raids', 'box', 'office', ...</td>\n",
       "      <td>['ocean', 'twelve', 'raid', 'box', 'office', '...</td>\n",
       "      <td>[ ' o c e a n ' ,   ' t w e l v e ' ,   ' r a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       category  \\\n",
       "0           0           tech   \n",
       "1           1       business   \n",
       "2           2          sport   \n",
       "3           3          sport   \n",
       "4           4  entertainment   \n",
       "\n",
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['tv', 'future', 'hands', 'viewers', 'home', '...   \n",
       "1  ['worldcom', 'boss', 'left', 'books', 'alone',...   \n",
       "2  ['tigers', 'wary', 'farrell', 'gamble', 'leice...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raids', 'box', 'office', ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['tv', 'future', 'hand', 'viewer', 'home', 'th...   \n",
       "1  ['worldcom', 'bos', 'leave', 'book', 'alone', ...   \n",
       "2  ['tiger', 'wary', 'farrell', 'gamble', 'leices...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raid', 'box', 'office', '...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [ ' t v ' ,   ' f u t u r e ' ,   ' h a n d ' ...  \n",
       "1  [ ' w o r l d c o m ' ,   ' b o s ' ,   ' l e ...  \n",
       "2  [ ' t i g e r ' ,   ' w a r y ' ,   ' f a r r ...  \n",
       "3  [ ' y e a d i n g ' ,   ' f a c e ' ,   ' n e ...  \n",
       "4  [ ' o c e a n ' ,   ' t w e l v e ' ,   ' r a ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmatized_text'] = data['lemmatized'].apply(lambda x: ' '.join(x)) #To convert the texts in the “lemmatized” column into single texts by joining the words using spaces.\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86cd4e30-8dbc-450b-8f24-3b127f0a2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['lemmatized_text'], errors='ignore', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a65a4f4-66ec-4708-b591-bd22d243d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aaa  aaas  aaron  abandon  abandonment  abbott  abc  abide  ability  \\\n",
      "0       0     0      0        0            0       0    0      1        0   \n",
      "1       0     0      0        0            0       0    0      0        1   \n",
      "2       0     0      0        0            0       0    0      0        0   \n",
      "3       0     0      0        0            0       0    0      0        0   \n",
      "4       0     0      0        0            0       0    0      0        0   \n",
      "...   ...   ...    ...      ...          ...     ...  ...    ...      ...   \n",
      "2220    0     0      0        0            0       0    0      0        0   \n",
      "2221    0     0      0        0            0       0    0      0        0   \n",
      "2222    0     0      0        0            0       0    0      0        0   \n",
      "2223    0     0      0        0            0       0    0      0        0   \n",
      "2224    0     0      0        0            0       0    0      0        0   \n",
      "\n",
      "      able  ...  zach  zealand  zeppelin  zero  zhang  zimbabwe  zombie  zone  \\\n",
      "0        0  ...     0        0         0     0      0         0       0     0   \n",
      "1        0  ...     0        0         0     0      0         0       0     0   \n",
      "2        0  ...     0        0         0     0      0         0       0     0   \n",
      "3        0  ...     0        0         0     0      0         0       0     0   \n",
      "4        0  ...     0        0         0     0      0         0       0     0   \n",
      "...    ...  ...   ...      ...       ...   ...    ...       ...     ...   ...   \n",
      "2220     0  ...     0        0         0     0      0         0       0     0   \n",
      "2221     0  ...     0        0         0     0      0         0       0     0   \n",
      "2222     0  ...     0        0         0     0      0         0       0     0   \n",
      "2223     0  ...     0        0         0     0      0         0       0     0   \n",
      "2224     0  ...     0        0         0     0      0         0       0     0   \n",
      "\n",
      "      zoom  zurich  \n",
      "0        0       0  \n",
      "1        0       0  \n",
      "2        0       0  \n",
      "3        0       0  \n",
      "4        0       0  \n",
      "...    ...     ...  \n",
      "2220     0       0  \n",
      "2221     0       0  \n",
      "2222     0       0  \n",
      "2223     0       0  \n",
      "2224     0       0  \n",
      "\n",
      "[2225 rows x 7085 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "#Bag of Words (BoW)\n",
    "bow_vectorizer = CountVectorizer(min_df=5)  \n",
    "bow_matrix = bow_vectorizer.fit_transform(data['lemmatized'])\n",
    "bow_df = pd.DataFrame.sparse.from_spmatrix(bow_matrix, columns=bow_vectorizer.get_feature_names_out())\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4df47011-3301-44cc-87cb-b7ce537469e3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#TF-IDF\u001b[39;00m\n\u001b[0;32m      5\u001b[0m tfidf_BBC \u001b[38;5;241m=\u001b[39m TfidfVectorizer(min_df\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;66;03m#Ignore rare words\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m tfidf_matrix \u001b[38;5;241m=\u001b[39m tfidf_BBC\u001b[38;5;241m.\u001b[39mfit_transform(data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;66;03m#tfidf_matrix-Represents the weight of each word based on its frequency.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m#Fit_transform-To convert texts to a matrix.\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m#Convert to DataFrame to display the data more clearly\u001b[39;00m\n\u001b[0;32m     10\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_BBC\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#TF-IDF\n",
    "tfidf_BBC = TfidfVectorizer(min_df=1)#Ignore rare words\n",
    "tfidf_matrix = tfidf_BBC.fit_transform(data['lemmatized'])#tfidf_matrix-Represents the weight of each word based on its frequency.\n",
    "#Fit_transform-To convert texts to a matrix.\n",
    "\n",
    "#Convert to DataFrame to display the data more clearly\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_BBC.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_df)\n",
    "with open(\"tfidf_BBC.pickle\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_BBC, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "514d352d-cb07-4ad6-9f2c-1cd35a52fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9685393258426966\n",
      "SVM Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.92      0.94       101\n",
      "entertainment       0.98      0.99      0.98        81\n",
      "     politics       0.94      0.96      0.95        83\n",
      "        sport       0.98      1.00      0.99        98\n",
      "         tech       0.99      0.98      0.98        82\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n",
      "Naive Bayes Accuracy: 0.9573033707865168\n",
      "Naive Bayes Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.94      0.94       101\n",
      "entertainment       1.00      0.89      0.94        81\n",
      "     politics       0.92      0.98      0.95        83\n",
      "        sport       0.99      1.00      0.99        98\n",
      "         tech       0.94      0.98      0.96        82\n",
      "\n",
      "     accuracy                           0.96       445\n",
      "    macro avg       0.96      0.96      0.96       445\n",
      " weighted avg       0.96      0.96      0.96       445\n",
      "\n",
      "Random Forest Accuracy: 0.946067415730337\n",
      "Random Forest Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.90      0.95      0.92       101\n",
      "entertainment       0.99      0.90      0.94        81\n",
      "     politics       0.94      0.94      0.94        83\n",
      "        sport       0.98      0.99      0.98        98\n",
      "         tech       0.94      0.94      0.94        82\n",
      "\n",
      "     accuracy                           0.95       445\n",
      "    macro avg       0.95      0.94      0.95       445\n",
      " weighted avg       0.95      0.95      0.95       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "# Prepare data for partitioning\n",
    "X = tfidf_matrix\n",
    "y = data['category'] \n",
    "\n",
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)  #Split data\n",
    "\n",
    "#Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)  #Make predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)  #Calculate accuracy\n",
    "    report = classification_report(y_test, predictions)  #Generate classification report\n",
    "    return accuracy, report\n",
    "\n",
    "svm_model = SVC(kernel='linear', class_weight='balanced')  #Initialize SVM model with linear kernel and class weights\n",
    "svm_model.fit(X_train, y_train)  #Train the model\n",
    "svm_accuracy, svm_report = evaluate_model(svm_model, X_test, y_test)  #Evaluate SVM\n",
    "print(\"SVM Accuracy:\", svm_accuracy)  #Print accuracy\n",
    "print(\"SVM Classification Report:\\n\", svm_report)  #Print classification report\n",
    "\n",
    "nb_model = MultinomialNB()  #Initialize Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)  #Train the model\n",
    "nb_accuracy, nb_report = evaluate_model(nb_model, X_test, y_test)  #Evaluate Naive Bayes\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)  #Print accuracy\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)  #Print classification report\n",
    "\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)  #Initialize Random Forest model with class weights\n",
    "rf_model.fit(X_train, y_train)  #Train the model\n",
    "rf_accuracy, rf_report = evaluate_model(rf_model, X_test, y_test)  #Evaluate Random Forest\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)  #Print accuracy\n",
    "print(\"Random Forest Classification Report:\\n\", rf_report)  #Print classification report\n",
    "with open(\"model_bbc.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(svm_model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "747062c2-8edd-4efd-9f1d-fe7907b3de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category is: business\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Function to preprocess text data\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s\\'\\\"]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Ensure there are no null or empty texts in the dataset\n",
    "data = data[data['lemmatized'].notnull() & (data['lemmatized'] != '')]\n",
    "\n",
    "# Preprocess the lemmatized text\n",
    "data['lemmatized_text'] = data['lemmatized'].apply(preprocess_data)\n",
    "\n",
    "# Create a TF-IDF matrix\n",
    "tfidf_BBC = TfidfVectorizer(min_df=5)  # Minimum document frequency of 5\n",
    "tfidf_matrix = tfidf_BBC.fit_transform(data['lemmatized_text'])\n",
    "\n",
    "# Prepare feature matrix (X) and target vector (y)\n",
    "X = tfidf_matrix\n",
    "y = data['category']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "lg_model = LogisticRegression(max_iter=200, verbose=1)  # Increase max iterations for convergence\n",
    "lg_model.fit(X_train, y_train)\n",
    "\n",
    "# Save the model and the TF-IDF vectorizer\n",
    "with open(\"model_bbc.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(lg_model, model_file)\n",
    "\n",
    "with open(\"tfidf_BBC.pickle\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_BBC, file)\n",
    "\n",
    "# Function to predict the category of a given text\n",
    "def predict_category(text):\n",
    "    cleaned_text = preprocess_data(text)  # Preprocess the input text\n",
    "    tfidf_data = tfidf_BBC.transform([cleaned_text])  # Convert text to TF-IDF features\n",
    "    prediction = lg_model.predict(tfidf_data)  # Predict category using the model\n",
    "    return prediction[0]  # Return the predicted category\n",
    "\n",
    "# Test the prediction function\n",
    "new_text = \"worldcom boss left books alone former worldcom boss bernie ebbers who is accused of overseeing an $11bn\"\n",
    "predicted_category = predict_category(new_text)\n",
    "print(f\"The predicted category is: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8fa8655-0a8f-4d41-9b15-c97b62d26aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# Load the trained model and TF-IDF vectorizer\n",
    "with open(\"model_bbc.pickle\", \"rb\") as model_file:\n",
    "    svm_model = pickle.load(model_file)\n",
    "\n",
    "with open(\"tfidf_BBC.pickle\", \"rb\") as file:\n",
    "    tfidf_BBC = pickle.load(file)\n",
    "\n",
    "# Function to preprocess text\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()  # Convert text to lowercase\n",
    "    text = re.sub(r'[^\\w\\s\\'\\\"]', '', text)  # Remove punctuation\n",
    "    return text\n",
    "\n",
    "# Function to classify the text using the loaded model\n",
    "def predict_category(text):\n",
    "    cleaned_text = preprocess_data(text)  # Preprocess the input text\n",
    "    tfidf_data = tfidf_BBC.transform([cleaned_text])  # Convert text to TF-IDF features\n",
    "    prediction = svm_model.predict(tfidf_data)  # Predict the category\n",
    "    return prediction[0]  # Return the predicted category\n",
    "\n",
    "# Function to classify the entered text and display the result\n",
    "def classify_text():\n",
    "    text = entry.get()  # Get the input text from the entry field\n",
    "    if not text.strip():  # Check if the text is empty\n",
    "        messagebox.showwarning(\"Error\", \"Please enter text for classification!\")  # Show a warning\n",
    "        return\n",
    "    category = predict_category(text)  # Predict the category\n",
    "    result_label.config(text=f\"Type of news: {category}\")  # Display the result\n",
    "\n",
    "# Function to clear the input text and reset the result label\n",
    "def clear_text():\n",
    "    entry.delete(0, tk.END)  # Clear the text entry\n",
    "    result_label.config(text=\"\")  # Reset the result label\n",
    "\n",
    "# Set up the main GUI window\n",
    "root = tk.Tk()\n",
    "root.geometry(\"800x500\")  # Set the window size\n",
    "root.title(\"BBC News Classification\")  # Set the window title\n",
    "\n",
    "# Add GUI elements\n",
    "label1 = tk.Label(root, text=\"News Classification Analyst\", font=(\"Helvetica\", 14))  # Main title\n",
    "label1.pack(pady=10)\n",
    "\n",
    "label2 = tk.Label(root, text=\"Enter text:\", font=(\"Helvetica\", 12))  # Instruction label\n",
    "label2.pack(pady=5)\n",
    "\n",
    "entry = tk.Entry(root, font=(\"Helvetica\", 12), width=50)  # Text entry field\n",
    "entry.pack(pady=5)\n",
    "\n",
    "classify_button = tk.Button(root, text=\"Classification\", command=classify_text, font=(\"Helvetica\", 12), bg=\"#002179\", fg=\"white\")\n",
    "classify_button.pack(pady=10)  # Button to classify text\n",
    "\n",
    "clear_button = tk.Button(root, text=\"Clear Text\", command=clear_text, font=(\"Helvetica\", 12), bg=\"#d9534f\", fg=\"white\")\n",
    "clear_button.pack(pady=5)  # Button to clear text\n",
    "\n",
    "result_label = tk.Label(root, text=\"\", font=(\"Helvetica\", 16), fg=\"blue\")  # Label to display the classification result\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "# Run the GUI application\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38eace4-bfa6-4b73-8935-d54be1afa9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
