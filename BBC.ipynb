{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5ea5d270-1bce-4c52-a3b9-78286ca4e876",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing libraries\n",
    "import pandas as pd  #Data manipulation\n",
    "import string  #Remove punctuation & characters\n",
    "import nltk  #Natural language processing \n",
    "import pickle  #For loading saved models and vectorizers\n",
    "import re\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords  #Stop word removal\n",
    "from nltk.tokenize import word_tokenize  #Tokenizition\n",
    "#from nltk.stem import PorterStemmer  #Stemming\n",
    "from nltk.stem import WordNetLemmatizer  #Import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet  #Import WordNet\n",
    "\n",
    "#Feature extractions libraries\n",
    "import string\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#Models libraries\n",
    "from sklearn.model_selection import train_test_split #For data splitting\n",
    "#Model Evaluation Function\n",
    "from sklearn.metrics import accuracy_score, classification_report  #Import metrics\n",
    "from sklearn.svm import SVC  #SVM Model\n",
    "from sklearn.naive_bayes import MultinomialNB #Naive Bayes Model -  MultinomialNB \n",
    "from sklearn.ensemble import RandomForestClassifier #Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b82144e6-6fc0-466d-a10f-35db9b8c6aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab4c6eef-f705-4164-87ca-781586a896e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>['tv', 'future', 'hands', 'viewers', 'home', '...</td>\n",
       "      <td>['tv', 'future', 'hand', 'viewer', 'home', 'th...</td>\n",
       "      <td>tv future hand viewer home theatre system plas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>['worldcom', 'boss', 'left', 'books', 'alone',...</td>\n",
       "      <td>['worldcom', 'bos', 'leave', 'book', 'alone', ...</td>\n",
       "      <td>worldcom bos leave book alone former worldcom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>['tigers', 'wary', 'farrell', 'gamble', 'leice...</td>\n",
       "      <td>['tiger', 'wary', 'farrell', 'gamble', 'leices...</td>\n",
       "      <td>tiger wary farrell gamble leicester say rush m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>yeading face newcastle fa cup premiership side...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>['ocean', 'twelve', 'raids', 'box', 'office', ...</td>\n",
       "      <td>['ocean', 'twelve', 'raid', 'box', 'office', '...</td>\n",
       "      <td>ocean twelve raid box office ocean twelve crim...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       category  \\\n",
       "0           0           tech   \n",
       "1           1       business   \n",
       "2           2          sport   \n",
       "3           3          sport   \n",
       "4           4  entertainment   \n",
       "\n",
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['tv', 'future', 'hands', 'viewers', 'home', '...   \n",
       "1  ['worldcom', 'boss', 'left', 'books', 'alone',...   \n",
       "2  ['tigers', 'wary', 'farrell', 'gamble', 'leice...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raids', 'box', 'office', ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['tv', 'future', 'hand', 'viewer', 'home', 'th...   \n",
       "1  ['worldcom', 'bos', 'leave', 'book', 'alone', ...   \n",
       "2  ['tiger', 'wary', 'farrell', 'gamble', 'leices...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raid', 'box', 'office', '...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  tv future hand viewer home theatre system plas...  \n",
       "1  worldcom bos leave book alone former worldcom ...  \n",
       "2  tiger wary farrell gamble leicester say rush m...  \n",
       "3  yeading face newcastle fa cup premiership side...  \n",
       "4  ocean twelve raid box office ocean twelve crim...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Read excel file\n",
    "data = pd.read_csv(r'C:\\Users\\s7r_2\\Downloads\\bbc-text (1).csv')\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e7b91d10-bf3e-4c72-9e1e-b94b667c2c7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  tv future in the hands of viewers with home th...  \n",
       "1  worldcom boss left books alone former worldcom...  \n",
       "2  tigers wary of farrell gamble leicester say th...  \n",
       "3  yeading face newcastle in fa cup premiership s...  \n",
       "4  ocean s twelve raids box office ocean s twelve...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# دالة لإزالة الرموز غير ASCII\n",
    "def _removeNonAscii(s): \n",
    "    return \"\".join(i for i in s if ord(i) < 128)\n",
    "\n",
    "# دالة لتنظيف النصوص\n",
    "def clean_text(text):\n",
    "    # تأكد من أن المتغير هو نص\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = text.replace('(ap)', '')\n",
    "    text = re.sub(r\"\\'s\", \" is \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r'\\W+', ' ', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r\"\\\\\", \"\", text)\n",
    "    text = re.sub(r\"\\'\", \"\", text)    \n",
    "    text = re.sub(r\"\\\"\", \"\", text)\n",
    "    text = re.sub('[^a-zA-Z ?!]+', '', text)\n",
    "    text = _removeNonAscii(text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# تحويل قائمة الكلمات إلى نص مفرد\n",
    "def list_to_string(tokens):\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# تأكد من وجود عمود 'tokens' كقوائم\n",
    "# تطبيق دالة تحويل القوائم إلى نصوص ثم دالة التنظيف على عمود tokens\n",
    "data['cleaned_text'] = data['text'].apply(clean_text)\n",
    "\n",
    "# عرض أول خمس صفوف من DataFrame بعد التنظيف\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d58964a-0807-4cd0-830e-646ef279f0fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \n",
       "0  [tv, future, hands, viewers, home, theatre, sy...  \n",
       "1  [worldcom, boss, left, books, alone, former, w...  \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...  \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# تحميل قائمة الكلمات الشائعة باللغة الإنجليزية\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# دالة لتوكنة النصوص وإزالة الكلمات الشائعة\n",
    "def tokenize_and_remove_stopwords(text):\n",
    "    if not isinstance(text, str):  # تحقق من أن المدخل هو نص\n",
    "        return []\n",
    "    \n",
    "    # توكنة النص\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    # إزالة الكلمات الشائعة\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "    \n",
    "    return filtered_tokens\n",
    "\n",
    "# تطبيق دالة توكنة النصوص وإزالة الكلمات الشائعة على عمود 'cleaned_text'\n",
    "data['tokens'] = data['cleaned_text'].apply(tokenize_and_remove_stopwords)\n",
    "# عرض أول خمس صفوف من DataFrame بعد معالجة النصوص\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3fb8f2ba-671b-44f2-9bc4-fb37f34fddf0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\s7r_2\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>[tv, future, hands, viewers, home, theatre, sy...</td>\n",
       "      <td>[tv, future, hand, viewer, home, theatre, syst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>[worldcom, boss, left, books, alone, former, w...</td>\n",
       "      <td>[worldcom, bos, leave, book, alone, former, wo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>[tigers, wary, farrell, gamble, leicester, say...</td>\n",
       "      <td>[tiger, wary, farrell, gamble, leicester, say,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "      <td>[yeading, face, newcastle, fa, cup, premiershi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>[ocean, twelve, raids, box, office, ocean, twe...</td>\n",
       "      <td>[ocean, twelve, raid, box, office, ocean, twel...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                               text  \\\n",
       "0           tech  tv future in the hands of viewers with home th...   \n",
       "1       business  worldcom boss  left books alone  former worldc...   \n",
       "2          sport  tigers wary of farrell  gamble  leicester say ...   \n",
       "3          sport  yeading face newcastle in fa cup premiership s...   \n",
       "4  entertainment  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  [tv, future, hands, viewers, home, theatre, sy...   \n",
       "1  [worldcom, boss, left, books, alone, former, w...   \n",
       "2  [tigers, wary, farrell, gamble, leicester, say...   \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...   \n",
       "4  [ocean, twelve, raids, box, office, ocean, twe...   \n",
       "\n",
       "                                          lemmatized  \n",
       "0  [tv, future, hand, viewer, home, theatre, syst...  \n",
       "1  [worldcom, bos, leave, book, alone, former, wo...  \n",
       "2  [tiger, wary, farrell, gamble, leicester, say,...  \n",
       "3  [yeading, face, newcastle, fa, cup, premiershi...  \n",
       "4  [ocean, twelve, raid, box, office, ocean, twel...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "# تحميل wordnet وملفات التصنيف النحوي\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# تهيئة WordNetLemmatizer\n",
    "wn = WordNetLemmatizer()\n",
    "\n",
    "# دالة لتحويل التصنيف النحوي من NLTK إلى WordNet\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # الافتراضي هو الاسم\n",
    "\n",
    "# دالة التجذير (lemmatizing) مع التصنيف النحوي\n",
    "def lemmatizing(tokens):\n",
    "    # تصنيف الكلمات النحوي\n",
    "    pos_tags = nltk.pos_tag(tokens)\n",
    "    # تطبيق التجذير باستخدام التصنيف النحوي\n",
    "    return [wn.lemmatize(word, get_wordnet_pos(tag)) for word, tag in pos_tags]\n",
    "\n",
    "# تطبيق دالة lemmatizing على عمود 'tokens'\n",
    "data['lemmatized'] = data['tokens'].apply(lemmatizing)\n",
    "\n",
    "# عرض أول خمس صفوف للتحقق من النتائج\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7626c459-0a9d-4e63-bf42-ebbb63ec2870",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>category</th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>tokens</th>\n",
       "      <th>lemmatized</th>\n",
       "      <th>lemmatized_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tech</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>tv future in the hands of viewers with home th...</td>\n",
       "      <td>['tv', 'future', 'hands', 'viewers', 'home', '...</td>\n",
       "      <td>['tv', 'future', 'hand', 'viewer', 'home', 'th...</td>\n",
       "      <td>[ ' t v ' ,   ' f u t u r e ' ,   ' h a n d ' ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom boss  left books alone  former worldc...</td>\n",
       "      <td>worldcom boss left books alone former worldcom...</td>\n",
       "      <td>['worldcom', 'boss', 'left', 'books', 'alone',...</td>\n",
       "      <td>['worldcom', 'bos', 'leave', 'book', 'alone', ...</td>\n",
       "      <td>[ ' w o r l d c o m ' ,   ' b o s ' ,   ' l e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>sport</td>\n",
       "      <td>tigers wary of farrell  gamble  leicester say ...</td>\n",
       "      <td>tigers wary of farrell gamble leicester say th...</td>\n",
       "      <td>['tigers', 'wary', 'farrell', 'gamble', 'leice...</td>\n",
       "      <td>['tiger', 'wary', 'farrell', 'gamble', 'leices...</td>\n",
       "      <td>[ ' t i g e r ' ,   ' w a r y ' ,   ' f a r r ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>sport</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>yeading face newcastle in fa cup premiership s...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>['yeading', 'face', 'newcastle', 'fa', 'cup', ...</td>\n",
       "      <td>[ ' y e a d i n g ' ,   ' f a c e ' ,   ' n e ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>entertainment</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>ocean s twelve raids box office ocean s twelve...</td>\n",
       "      <td>['ocean', 'twelve', 'raids', 'box', 'office', ...</td>\n",
       "      <td>['ocean', 'twelve', 'raid', 'box', 'office', '...</td>\n",
       "      <td>[ ' o c e a n ' ,   ' t w e l v e ' ,   ' r a ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0       category  \\\n",
       "0           0           tech   \n",
       "1           1       business   \n",
       "2           2          sport   \n",
       "3           3          sport   \n",
       "4           4  entertainment   \n",
       "\n",
       "                                                text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss  left books alone  former worldc...   \n",
       "2  tigers wary of farrell  gamble  leicester say ...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  tv future in the hands of viewers with home th...   \n",
       "1  worldcom boss left books alone former worldcom...   \n",
       "2  tigers wary of farrell gamble leicester say th...   \n",
       "3  yeading face newcastle in fa cup premiership s...   \n",
       "4  ocean s twelve raids box office ocean s twelve...   \n",
       "\n",
       "                                              tokens  \\\n",
       "0  ['tv', 'future', 'hands', 'viewers', 'home', '...   \n",
       "1  ['worldcom', 'boss', 'left', 'books', 'alone',...   \n",
       "2  ['tigers', 'wary', 'farrell', 'gamble', 'leice...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raids', 'box', 'office', ...   \n",
       "\n",
       "                                          lemmatized  \\\n",
       "0  ['tv', 'future', 'hand', 'viewer', 'home', 'th...   \n",
       "1  ['worldcom', 'bos', 'leave', 'book', 'alone', ...   \n",
       "2  ['tiger', 'wary', 'farrell', 'gamble', 'leices...   \n",
       "3  ['yeading', 'face', 'newcastle', 'fa', 'cup', ...   \n",
       "4  ['ocean', 'twelve', 'raid', 'box', 'office', '...   \n",
       "\n",
       "                                     lemmatized_text  \n",
       "0  [ ' t v ' ,   ' f u t u r e ' ,   ' h a n d ' ...  \n",
       "1  [ ' w o r l d c o m ' ,   ' b o s ' ,   ' l e ...  \n",
       "2  [ ' t i g e r ' ,   ' w a r y ' ,   ' f a r r ...  \n",
       "3  [ ' y e a d i n g ' ,   ' f a c e ' ,   ' n e ...  \n",
       "4  [ ' o c e a n ' ,   ' t w e l v e ' ,   ' r a ...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['lemmatized_text'] = data['lemmatized'].apply(lambda x: ' '.join(x)) #To convert the texts in the “lemmatized” column into single texts by joining the words using spaces.\n",
    "data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "86cd4e30-8dbc-450b-8f24-3b127f0a2d83",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(columns=['lemmatized_text'], errors='ignore', inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a65a4f4-66ec-4708-b591-bd22d243d518",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      aaa  aaas  aaron  abandon  abandonment  abbott  abc  abide  ability  \\\n",
      "0       0     0      0        0            0       0    0      1        0   \n",
      "1       0     0      0        0            0       0    0      0        1   \n",
      "2       0     0      0        0            0       0    0      0        0   \n",
      "3       0     0      0        0            0       0    0      0        0   \n",
      "4       0     0      0        0            0       0    0      0        0   \n",
      "...   ...   ...    ...      ...          ...     ...  ...    ...      ...   \n",
      "2220    0     0      0        0            0       0    0      0        0   \n",
      "2221    0     0      0        0            0       0    0      0        0   \n",
      "2222    0     0      0        0            0       0    0      0        0   \n",
      "2223    0     0      0        0            0       0    0      0        0   \n",
      "2224    0     0      0        0            0       0    0      0        0   \n",
      "\n",
      "      able  ...  zach  zealand  zeppelin  zero  zhang  zimbabwe  zombie  zone  \\\n",
      "0        0  ...     0        0         0     0      0         0       0     0   \n",
      "1        0  ...     0        0         0     0      0         0       0     0   \n",
      "2        0  ...     0        0         0     0      0         0       0     0   \n",
      "3        0  ...     0        0         0     0      0         0       0     0   \n",
      "4        0  ...     0        0         0     0      0         0       0     0   \n",
      "...    ...  ...   ...      ...       ...   ...    ...       ...     ...   ...   \n",
      "2220     0  ...     0        0         0     0      0         0       0     0   \n",
      "2221     0  ...     0        0         0     0      0         0       0     0   \n",
      "2222     0  ...     0        0         0     0      0         0       0     0   \n",
      "2223     0  ...     0        0         0     0      0         0       0     0   \n",
      "2224     0  ...     0        0         0     0      0         0       0     0   \n",
      "\n",
      "      zoom  zurich  \n",
      "0        0       0  \n",
      "1        0       0  \n",
      "2        0       0  \n",
      "3        0       0  \n",
      "4        0       0  \n",
      "...    ...     ...  \n",
      "2220     0       0  \n",
      "2221     0       0  \n",
      "2222     0       0  \n",
      "2223     0       0  \n",
      "2224     0       0  \n",
      "\n",
      "[2225 rows x 7085 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  # تأكد من استيراد CountVectorizer\n",
    "\n",
    "#Bag of Words (BoW)\n",
    "bow_vectorizer = CountVectorizer(min_df=5)  # يمكنك تجربة قيم أعلى حسب حجم البيانات\n",
    "bow_matrix = bow_vectorizer.fit_transform(data['lemmatized'])\n",
    "bow_df = pd.DataFrame.sparse.from_spmatrix(bow_matrix, columns=bow_vectorizer.get_feature_names_out())\n",
    "print(bow_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4df47011-3301-44cc-87cb-b7ce537469e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      "       aa  aaa  aaas  aac  aadc  aaliyah  aaltra  aamir  aan  aara  ...  zoom  \\\n",
      "0     0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "1     0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "2     0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "3     0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "4     0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "...   ...  ...   ...  ...   ...      ...     ...    ...  ...   ...  ...   ...   \n",
      "2220  0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "2221  0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "2222  0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "2223  0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "2224  0.0  0.0   0.0  0.0   0.0      0.0     0.0    0.0  0.0   0.0  ...   0.0   \n",
      "\n",
      "      zooropa  zornotza  zorro  zubair  zuluaga  zurich  zutons  zvonareva  \\\n",
      "0         0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "1         0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "2         0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "3         0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "4         0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "...       ...       ...    ...     ...      ...     ...     ...        ...   \n",
      "2220      0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "2221      0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "2222      0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "2223      0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "2224      0.0       0.0    0.0     0.0      0.0     0.0     0.0        0.0   \n",
      "\n",
      "      zvyagintsev  \n",
      "0             0.0  \n",
      "1             0.0  \n",
      "2             0.0  \n",
      "3             0.0  \n",
      "4             0.0  \n",
      "...           ...  \n",
      "2220          0.0  \n",
      "2221          0.0  \n",
      "2222          0.0  \n",
      "2223          0.0  \n",
      "2224          0.0  \n",
      "\n",
      "[2225 rows x 23136 columns]\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "#TF-IDF\n",
    "tfidf_BBC = TfidfVectorizer(min_df=1)#Ignore rare words\n",
    "tfidf_matrix = tfidf_BBC.fit_transform(data['lemmatized'])#tfidf_matrix-Represents the weight of each word based on its frequency.\n",
    "#Fit_transform-To convert texts to a matrix.\n",
    "\n",
    "#Convert to DataFrame to display the data more clearly\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_BBC.get_feature_names_out())\n",
    "\n",
    "print(\"\\nTF-IDF Matrix:\")\n",
    "print(tfidf_df)\n",
    "with open(\"tfidf_BBC.pickle\", \"wb\") as file:\n",
    "    pickle.dump(tfidf_BBC, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "514d352d-cb07-4ad6-9f2c-1cd35a52fba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy: 0.9685393258426966\n",
      "SVM Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.96      0.92      0.94       101\n",
      "entertainment       0.98      0.99      0.98        81\n",
      "     politics       0.94      0.96      0.95        83\n",
      "        sport       0.98      1.00      0.99        98\n",
      "         tech       0.99      0.98      0.98        82\n",
      "\n",
      "     accuracy                           0.97       445\n",
      "    macro avg       0.97      0.97      0.97       445\n",
      " weighted avg       0.97      0.97      0.97       445\n",
      "\n",
      "Naive Bayes Accuracy: 0.9573033707865168\n",
      "Naive Bayes Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.94      0.94      0.94       101\n",
      "entertainment       1.00      0.89      0.94        81\n",
      "     politics       0.92      0.98      0.95        83\n",
      "        sport       0.99      1.00      0.99        98\n",
      "         tech       0.94      0.98      0.96        82\n",
      "\n",
      "     accuracy                           0.96       445\n",
      "    macro avg       0.96      0.96      0.96       445\n",
      " weighted avg       0.96      0.96      0.96       445\n",
      "\n",
      "Random Forest Accuracy: 0.946067415730337\n",
      "Random Forest Classification Report:\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "     business       0.90      0.95      0.92       101\n",
      "entertainment       0.99      0.90      0.94        81\n",
      "     politics       0.94      0.94      0.94        83\n",
      "        sport       0.98      0.99      0.98        98\n",
      "         tech       0.94      0.94      0.94        82\n",
      "\n",
      "     accuracy                           0.95       445\n",
      "    macro avg       0.95      0.94      0.95       445\n",
      " weighted avg       0.95      0.95      0.95       445\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pickle\n",
    "\n",
    "# إعداد البيانات للتقسيم\n",
    "X = tfidf_matrix\n",
    "y = data['category'] \n",
    "\n",
    "#Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, y, test_size=0.2, random_state=42)  #Split data\n",
    "\n",
    "#Function to evaluate model performance\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    predictions = model.predict(X_test)  #Make predictions\n",
    "    accuracy = accuracy_score(y_test, predictions)  #Calculate accuracy\n",
    "    report = classification_report(y_test, predictions)  #Generate classification report\n",
    "    return accuracy, report\n",
    "\n",
    "\n",
    "\n",
    "svm_model = SVC(kernel='linear', class_weight='balanced')  #Initialize SVM model with linear kernel and class weights\n",
    "svm_model.fit(X_train, y_train)  #Train the model\n",
    "svm_accuracy, svm_report = evaluate_model(svm_model, X_test, y_test)  #Evaluate SVM\n",
    "print(\"SVM Accuracy:\", svm_accuracy)  #Print accuracy\n",
    "print(\"SVM Classification Report:\\n\", svm_report)  #Print classification report\n",
    "\n",
    "\n",
    "nb_model = MultinomialNB()  #Initialize Naive Bayes model\n",
    "nb_model.fit(X_train, y_train)  #Train the model\n",
    "nb_accuracy, nb_report = evaluate_model(nb_model, X_test, y_test)  #Evaluate Naive Bayes\n",
    "print(\"Naive Bayes Accuracy:\", nb_accuracy)  #Print accuracy\n",
    "print(\"Naive Bayes Classification Report:\\n\", nb_report)  #Print classification report\n",
    "\n",
    "\n",
    "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42)  #Initialize Random Forest model with class weights\n",
    "rf_model.fit(X_train, y_train)  #Train the model\n",
    "rf_accuracy, rf_report = evaluate_model(rf_model, X_test, y_test)  #Evaluate Random Forest\n",
    "print(\"Random Forest Accuracy:\", rf_accuracy)  #Print accuracy\n",
    "print(\"Random Forest Classification Report:\\n\", rf_report)  #Print classification report\n",
    "with open(\"model_bbc.pickle\", \"wb\") as model_file:\n",
    "    pickle.dump(svm_model, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "747062c2-8edd-4efd-9f1d-fe7907b3de88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The predicted category is: business\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# دالة معالجة البيانات\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()  # تحويل النص إلى حروف صغيرة\n",
    "    text = re.sub(r'[^\\w\\s\\'\\\"]', '', text)  # إزالة علامات الترقيم\n",
    "    return text\n",
    "\n",
    "# التأكد من عدم وجود نصوص فارغة\n",
    "data = data[data['lemmatized_text'].notnull() & (data['lemmatized_text'] != '')]\n",
    "\n",
    "# معالجة البيانات\n",
    "data['lemmatized_text'] = data['lemmatized_text'].apply(preprocess_data)\n",
    "\n",
    "# إنشاء مصفوفة TF-IDF\n",
    "tfidf_BBC = TfidfVectorizer(min_df=5)\n",
    "tfidf_matrix = tfidf_BBC.fit_transform(data['lemmatized'])\n",
    "\n",
    "# إعداد البيانات\n",
    "X = tfidf_matrix\n",
    "y = data['category']\n",
    "\n",
    "# تقسيم البيانات\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# تدريب نموذج الانحدار اللوجستي\n",
    "lg_model = LogisticRegression(max_iter=200, verbose=1)\n",
    "lg_model.fit(X_train, y_train)\n",
    "\n",
    "# حفظ النموذج وtfidf_vectorizer\n",
    "with open(\"model_bbc.pickle\", \"rb\") as model_file:\n",
    "    svm_model = pickle.load(model_file)\n",
    "\n",
    "with open(\"tfidf_BBC.pickle\", \"rb\") as file:\n",
    "    tfidf_BBC = pickle.load(file)\n",
    "\n",
    "\n",
    "    \n",
    "# دالة التنبؤ\n",
    "def predict_category(text):\n",
    "    cleaned_text = preprocess_data(text)  # معالجة النص\n",
    "    tfidf_data = tfidf_BBC.transform([cleaned_text])  # تحويل النص إلى تمثيل TF-IDF\n",
    "    prediction = svm_model.predict(tfidf_data)  # إجراء التنبؤ\n",
    "    return prediction[0]  # إرجاع الفئة المتوقعة\n",
    "\n",
    "\n",
    "# اختبار دالة التنبؤ\n",
    "new_text = \"worldcom boss  left books alone  former worldcom boss bernie ebbers  who is accused of overseeing an $11bn\"\n",
    "predicted_category = predict_category(new_text)\n",
    "print(f\"The predicted category is: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fa8655-0a8f-4d41-9b15-c97b62d26aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "import tkinter as tk\n",
    "from tkinter import messagebox\n",
    "\n",
    "# تحميل النموذج و TF-IDF Vectorizer\n",
    "with open(\"model_bbc.pickle\", \"rb\") as model_file:\n",
    "    svm_model = pickle.load(model_file)\n",
    "\n",
    "with open(\"tfidf_BBC.pickle\", \"rb\") as file:\n",
    "    tfidf_BBC = pickle.load(file)\n",
    "\n",
    "# دالة المعالجة المسبقة للنصوص\n",
    "def preprocess_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s\\'\\\"]', '', text)\n",
    "    return text\n",
    "\n",
    "# دالة التصنيف باستخدام النموذج المحمل\n",
    "def predict_category(text):\n",
    "    cleaned_text = preprocess_data(text)  # معالجة النص\n",
    "    tfidf_data = tfidf_BBC.transform([cleaned_text])  # تحويل النص إلى تمثيل TF-IDF\n",
    "    prediction = svm_model.predict(tfidf_data)  # إجراء التنبؤ\n",
    "    return prediction[0]  # إرجاع الفئة المتوقعة\n",
    "\n",
    "\n",
    "# دالة لتصنيف النص وإظهار النتيجة\n",
    "def classify_text():\n",
    "    text = entry.get()\n",
    "    if not text.strip():\n",
    "        messagebox.showwarning(\"خطأ\", \"الرجاء إدخال نص للتصنيف!\")\n",
    "        return\n",
    "    category = predict_category(text)\n",
    "    result_label.config(text=f\"نوع الخبر: {category}\")\n",
    "\n",
    "# دالة لمسح النص\n",
    "def clear_text():\n",
    "    entry.delete(0, tk.END)\n",
    "    result_label.config(text=\"\")\n",
    "\n",
    "# إعداد النافذة الرئيسية\n",
    "root = tk.Tk()\n",
    "root.geometry(\"800x500\")\n",
    "root.title(\"News Classification\")\n",
    "\n",
    "# إعداد العناصر في واجهة المستخدم\n",
    "label1 = tk.Label(root, text=\"محلل تصنيف الأخبار\", font=(\"Helvetica\", 14))\n",
    "label1.pack(pady=10)\n",
    "\n",
    "label2 = tk.Label(root, text=\"أدخل النص:\", font=(\"Helvetica\", 12))\n",
    "label2.pack(pady=5)\n",
    "\n",
    "entry = tk.Entry(root, font=(\"Helvetica\", 12), width=50)\n",
    "entry.pack(pady=5)\n",
    "\n",
    "classify_button = tk.Button(root, text=\"تصنيف\", command=classify_text, font=(\"Helvetica\", 12), bg=\"#002179\", fg=\"white\")\n",
    "classify_button.pack(pady=10)\n",
    "\n",
    "clear_button = tk.Button(root, text=\"مسح النص\", command=clear_text, font=(\"Helvetica\", 12), bg=\"#d9534f\", fg=\"white\")\n",
    "clear_button.pack(pady=5)\n",
    "\n",
    "result_label = tk.Label(root, text=\"\", font=(\"Helvetica\", 16), fg=\"blue\")\n",
    "result_label.pack(pady=20)\n",
    "\n",
    "# بدء تشغيل النافذة\n",
    "root.mainloop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38eace4-bfa6-4b73-8935-d54be1afa9cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
